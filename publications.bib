# Doctoral consortium

@inproceedings{lambert_doctoral_consortium,
author = {Lambert, Vincent},
title = {Representing microgestures interaction for wearable computing},
year = {2023},
isbn = {9781450399241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565066.3609740},
doi = {10.1145/3565066.3609740},
abstract = {Microgestures, i.e. fast and subtle movements of the fingers, offer always available and versatile interaction that makes them promising for wearable computing. While research has been done to define sets of microgestures and sensing systems, we do not know how to best represent microgestures and their associated commands which is essential for the adoption of microgesture interaction. Thus, this thesis focuses on how microgestures and their corresponding commands can be represented to users and discovered by them.},
booktitle = {Proceedings of the 25th International Conference on Mobile Human-Computer Interaction},
articleno = {40},
numpages = {4},
keywords = {Visual cues;Representations., Microgestures, Command exploration},
location = {Athens, Greece},
series = {MobileHCI '23 Companion}
}

# Full papers

@article{lambert_visual_2023,
author = {Lambert, Vincent and Chaffangeon Caillet, Adrien and Goguey, Alix and Malacria, Sylvain and Nigay, Laurence},
title = {Studying the Visual Representation of Microgestures},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {MHCI},
url = {https://doi.org/10.1145/3604272},
doi = {10.1145/3604272},
abstract = {The representations of microgestures are essentials for researchers presenting their results through academic papers and system designers proposing tutorials to novice users. However, those representations remain disparate and inconsistent. As a first attempt to investigate how to best graphically represent microgestures, we created 21 designs, each depicting static and dynamic versions of 4 commonly used microgestures (tap, swipe, flex and hold). We first studied these designs in a quantitative online experiment with 45 participants. We then conducted a qualitative laboratory experiment in Augmented Reality with 16 participants. Based on the results, we provide design guidelines on which elements of a microgesture should be represented and how. In particular, it is recommended to represent the actuator and the trajectory of a microgesture. Also, although preferred by users, dynamic representations are not considered better than their static counterparts for depicting a microgesture and do not necessarily result in a better user recognition.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = sep,
articleno = {225},
numpages = {36},
keywords = {microgesture representations, microgesture, discoverability, AR}
}

@article{lambert_simultaneous_2024,
author = {Lambert, Vincent and Goguey, Alix and Malacria, Sylvain and Nigay, Laurence},
title = {Studying the Simultaneous Visual Representation of Microgestures},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {MHCI},
url = {https://doi.org/10.1145/3676523},
doi = {10.1145/3676523},
abstract = {Hand microgestures are promising for mobile interaction with wearable devices. However, they will not be adopted if practitioners cannot communicate to users the microgestures associated with the commands of their applications. This requires unambiguous representations that simultaneously show the multiple microgestures available to control an application. Using a systematic approach, we evaluate how these representations should be designed and contrast 4 conditions depending on the microgestures (tap-swipe and tap-hold) and fingers (index and index-middle) considered. Based on the results, we design a simultaneous representation of microgestures for a given set of 14 application commands. We then evaluate the usability of the representation for novice users and the suitability of the representation for small screens compared with a baseline. Finally, we formulate 8 recommendations based on the results of all the experiments. In particular, redundant graphical and textual representations of microgestures should only be displayed for novice users.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = sep,
articleno = {276},
numpages = {34},
keywords = {command, microgesture, representation of microgestures}
}

@inproceedings{chaffangeoncaillet:hal-05311866,
  TITLE = {{Microgesture Interaction in Context: demonstrations of the ANR MIC project Interaction par microgeste en contexte : d{\'e}monstrations du projet ANR MIC}},
  AUTHOR = {Chaffangeon Caillet, Adrien and Conil, Aur{\'e}lien and Goguey, Alix and Lambert, Vincent and Nigay, Laurence and Bailly, Charles and Castet, Julien and Ortega, Michael and Lacroux, Zo{\'e} and Lemercier, C{\'e}line and Paubel, Pierre-Vincent and Bardot, Sandra and Jouffrais, Christophe and Lavenant, Suliac and Malacria, Sylvain and Pietrzak, Thomas},
  URL = {https://hal.science/hal-05311866},
  BOOKTITLE = {{IHM'25 : Actes {\'e}tendus de la 36{\`e}me conf{\'e}rence Francophone sur l'Interaction Humain-Machine}},
  ADDRESS = {Toulouse, France},
  HAL_LOCAL_REFERENCE = {Demo},
  YEAR = {2025},
  MONTH = Nov,
  KEYWORDS = {In-vehicle Interaction ; Microgesture ; Toolkit ; Microgesture recognition ; Augmented Reality ; Accessibility ; Visual Impairment ; Bo{\^i}te {\`a} outils ; Microgestes ; D{\'e}ficience visuelle ; Accessibilit{\'e} ; Interaction en Voiture ; R{\'e}alit{\'e} Augment{\'e}e ; Reconnaissance de microgestes},
  PDF = {https://hal.science/hal-05311866v1/file/DemoIHM2025-ChaffangeonCaillet-article.pdf},
  HAL_ID = {hal-05311866},
  HAL_VERSION = {v1},
}
